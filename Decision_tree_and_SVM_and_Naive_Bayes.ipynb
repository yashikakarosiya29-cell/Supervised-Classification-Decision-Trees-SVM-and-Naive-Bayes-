{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# SVM and Naive bayes"
      ],
      "metadata": {
        "id": "ay0NxBirOwGS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. What is Information Gain, and how is it used in Decision Trees?\n",
        " -  Information Gain is a metric used in Decision Tree algorithms to select the best feature for splitting the dataset.\n",
        "It measures the reduction in uncertainty (entropy) after splitting the data on a particular feature.\n",
        "\n",
        "Entropy represents the level of impurity or randomness in the dataset.\n",
        "When a dataset is split using a feature, Information Gain calculates how much the entropy decreases.\n",
        "\n",
        "Formula:\n",
        "Information Gain(S, A) = Entropy(S) − Σ (|Sv| / |S|) × Entropy(Sv)\n",
        "\n",
        "Where:\n",
        "S = total dataset\n",
        "A = attribute (feature)\n",
        "Sv = subset of S after splitting on attribute A\n",
        "\n",
        "In Decision Trees, the feature with the highest Information Gain is chosen for splitting at each node because it provides the most useful information for classification.\n"
      ],
      "metadata": {
        "id": "EA0JeK6yOy0f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2: What is the difference between Gini Impurity and Entropy? Hint: Directly compares the two main impurity measures, highlighting strengths, weaknesses, and appropriate use cases.\n",
        "-  Gini Impurity and Entropy are both impurity measures used in Decision Tree algorithms to evaluate the quality of a split.\n",
        "\n",
        "Entropy measures the level of uncertainty or randomness in the dataset and is based on information theory.\n",
        "It uses logarithmic calculations, which makes it computationally more expensive but more sensitive to changes in class probabilities.\n",
        "\n",
        "Gini Impurity measures the probability of incorrect classification of a randomly chosen data point.\n",
        "It is computationally faster than Entropy and is less sensitive to small probability changes.\n",
        "\n",
        "Entropy is mainly used in ID3 and C4.5 algorithms, while Gini Impurity is used in the CART algorithm.\n",
        "In practice, both give similar results, but Gini is preferred when speed is important, and Entropy is preferred when detailed information gain is required.\n"
      ],
      "metadata": {
        "id": "-FlGDvGrOytB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 3:What is Pre-Pruning in Decision Trees?\n",
        "-  Pre-Pruning is a technique used in Decision Trees to prevent overfitting by stopping the growth of the tree at an early stage.\n",
        "Instead of allowing the tree to grow fully, certain conditions are applied before splitting a node.\n",
        "\n",
        "Common pre-pruning criteria include setting a maximum tree depth, minimum number of samples required to split a node,\n",
        "minimum information gain, or minimum samples in a leaf node.\n",
        "\n",
        "By stopping unnecessary splits, pre-pruning reduces model complexity, improves generalization, and decreases training time.\n",
        "However, excessive pre-pruning may lead to underfitting if the tree stops growing too early.\n"
      ],
      "metadata": {
        "id": "qF7TSFUAS3Y9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4:Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances (practical). Hint: Use criterion='gini' in DecisionTreeClassifier and access .feature_importances_. (Include your Python code and output in the code box below.)\n",
        "-"
      ],
      "metadata": {
        "id": "M5bK87hfS4Ib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train Decision Tree Classifier using Gini Impurity\n",
        "model = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for feature, importance in zip(data.feature_names, model.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "In5s5tWbToPN",
        "outputId": "e0abf982-3297-4797-f8c3-1dc094798136"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances:\n",
            "sepal length (cm): 0.0133\n",
            "sepal width (cm): 0.0000\n",
            "petal length (cm): 0.5641\n",
            "petal width (cm): 0.4226\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 5: What is a Support Vector Machine (SVM)?\n",
        "-  A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks.\n",
        "It works by finding an optimal decision boundary called a hyperplane that best separates the data points of different classes.\n",
        "\n",
        "SVM aims to maximize the margin, which is the distance between the hyperplane and the nearest data points from each class,\n",
        "known as support vectors. These support vectors play a key role in defining the position of the hyperplane.\n",
        "\n",
        "SVM can handle both linear and non-linear data. For non-linear classification, it uses kernel functions such as linear,\n",
        "polynomial, and radial basis function (RBF) to transform data into a higher-dimensional space.\n",
        "\n",
        "Due to its ability to create robust decision boundaries, SVM is effective in high-dimensional spaces and is less prone\n",
        "to overfitting when properly tuned.\n"
      ],
      "metadata": {
        "id": "fqqNwCIUTv42"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 6:  What is the Kernel Trick in SVM?\n",
        "-   The Kernel Trick is a technique used in Support Vector Machines (SVM) to handle non-linear data.\n",
        "It allows SVM to separate data that is not linearly separable by implicitly mapping it into a higher-dimensional feature space.\n",
        "\n",
        "Instead of explicitly computing the transformation, the kernel trick uses kernel functions to calculate the inner products\n",
        "between data points in the higher-dimensional space directly. This makes the computation efficient.\n",
        "\n",
        "Common kernel functions include Linear, Polynomial, and Radial Basis Function (RBF).\n",
        "By using an appropriate kernel, SVM can create a linear decision boundary in the transformed space that corresponds to a\n",
        "non-linear boundary in the original space.\n",
        "\n",
        "Thus, the kernel trick enables SVM to solve complex non-linear classification problems without increasing computational cost significantly.\n"
      ],
      "metadata": {
        "id": "kxgQZxGLTvz6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 7:  Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies. Hint:Use SVC(kernel='linear') and SVC(kernel='rbf'), then compare accuracy scores after fitting on the same dataset. (Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "XAOiL6vXTvtm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train SVM with Linear Kernel\n",
        "svm_linear = SVC(kernel='linear')\n",
        "svm_linear.fit(X_train, y_train)\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "linear_accuracy = accuracy_score(y_test, y_pred_linear)\n",
        "\n",
        "# Train SVM with RBF Kernel\n",
        "svm_rbf = SVC(kernel='rbf')\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "rbf_accuracy = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "# Print accuracy results\n",
        "print(\"Accuracy using Linear Kernel SVM:\", linear_accuracy)\n",
        "print(\"Accuracy using RBF Kernel SVM:\", rbf_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZphuZFRUyq2",
        "outputId": "5b5d7e72-c264-4466-b421-7ac3ca83be6d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy using Linear Kernel SVM: 0.9814814814814815\n",
            "Accuracy using RBF Kernel SVM: 0.7592592592592593\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 8: What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "- The Naïve Bayes classifier is a supervised machine learning algorithm based on Bayes’ Theorem.\n",
        "It is mainly used for classification tasks such as text classification, spam detection, and sentiment analysis.\n",
        "\n",
        "It is called “Naïve” because it assumes that all features are independent of each other given the class label.\n",
        "This independence assumption is usually unrealistic in real-world data, but it simplifies the computation.\n",
        "\n",
        "Despite this strong assumption, Naïve Bayes performs well in many practical applications, especially with large datasets\n",
        "and high-dimensional data. It is fast, efficient, and works well even with limited training data.\n"
      ],
      "metadata": {
        "id": "rN0tStstTvoL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 9: Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes ?\n",
        "-  Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes are different variants of the Naïve Bayes classifier,\n",
        "each designed for different types of data.\n",
        "\n",
        "Gaussian Naïve Bayes is used when the features are continuous and are assumed to follow a normal (Gaussian) distribution.\n",
        "It is commonly applied in problems involving numerical data such as measurements and sensor values.\n",
        "\n",
        "Multinomial Naïve Bayes is used for discrete count-based features.\n",
        "It is widely used in text classification problems where features represent word counts or term frequencies.\n",
        "\n",
        "Bernoulli Naïve Bayes is used for binary features, where each feature represents the presence or absence of a characteristic.\n",
        "It is suitable for binary text features such as whether a word appears in a document or not.\n",
        "\n",
        "Thus, the main difference between these Naïve Bayes variants lies in the type of data they are designed to handle.\n"
      ],
      "metadata": {
        "id": "_db60IRKVOcK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 10:  Breast Cancer Dataset Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and evaluate accuracy. Hint:Use GaussianNB() from sklearn.naive_bayes and the Breast Cancer dataset from sklearn.datasets. (Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "NzwfetrVViMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train Gaussian Naïve Bayes classifier\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy of Gaussian Naïve Bayes on Breast Cancer dataset:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lm4Wm5VnV3AQ",
        "outputId": "2a66d930-4e06-47ed-db6e-a3d314a243f0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Gaussian Naïve Bayes on Breast Cancer dataset: 0.9415204678362573\n"
          ]
        }
      ]
    }
  ]
}